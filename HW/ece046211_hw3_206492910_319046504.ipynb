{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzV9wsJ5pGhf"
      },
      "source": [
        "# <img src=\"https://img.icons8.com/bubbles/50/000000/mind-map.png\" style=\"height:50px;display:inline\"> ECE 046211 - Technion - Deep Learning\n",
        "---\n",
        "\n",
        "## HW3 - Sequential Tasks and Training Methods\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2c8X93pGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/clouds/96/000000/keyboard.png\" style=\"height:50px;display:inline\"> Keyboard Shortcuts\n",
        "---\n",
        "* Run current cell: **Ctrl + Enter**\n",
        "* Run current cell and move to the next: **Shift + Enter**\n",
        "* Show lines in a code cell: **Esc + L**\n",
        "* View function documentation: **Shift + Tab** inside the parenthesis or `help(name_of_module)`\n",
        "* New cell below: **Esc + B**\n",
        "* Delete cell: **Esc + D, D** (two D's)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZybn3NpGhh"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/information.png\" style=\"height:50px;display:inline\"> Students Information\n",
        "---\n",
        "* Fill in\n",
        "\n",
        "|Name     |Campus Email| ID  |\n",
        "|---------|--------------------------------|----------|\n",
        "|Lavie Lederman| laviel@campus.technion.ac.il| 319046504|\n",
        "|Barry Shafran| barryshafran@campus.technion.ac.il| 206492910|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDK5zqhdpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/upload-to-cloud.png\" style=\"height:50px;display:inline\"> Submission Guidelines\n",
        "---\n",
        "* Maximal garde: 100.\n",
        "* Submission only in **pairs**.\n",
        "    * Please make sure you have registered your group in Moodle (there is a group creation component on the Moodle where you need to create your group and assign members).\n",
        "* **No handwritten submissions.** You can choose whether to answer in a Markdown cell in this notebook or attach a PDF with your answers.\n",
        "* <a style='color:red'> SAVE THE NOTEBOOKS WITH THE OUTPUT, CODE CELLS THAT WERE NOT RUN WILL NOT GET ANY POINTS! </a>\n",
        "* What you have to submit:\n",
        "    * If you have answered the questions in the notebook, you should submit this file only, with the name: `ece046211_hw3_id1_id2.ipynb`.\n",
        "    * If you answered the questionss in a different file you should submit a `.zip` file with the name `ece046211_hw3_id1_id2.zip` with content:\n",
        "        * `ece046211_hw3_id1_id2.ipynb` - the code tasks\n",
        "        * `ece046211_hw3_id1_id2.pdf` - answers to questions.\n",
        "    * No other file-types (`.py`, `.docx`...) will be accepted.\n",
        "* Submission on the course website (Moodle).\n",
        "* **Latex in Colab** - in some cases, Latex equations may no be rendered. To avoid this, make sure to not use *bullets* in your answers (\"* some text here with Latex equations\" -> \"some text here with Latex equations\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmSj_UufpGhi"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/dusk/64/000000/online.png\" style=\"height:50px;display:inline\"> Working Online and Locally\n",
        "---\n",
        "* You can choose your working environment:\n",
        "    1. `Jupyter Notebook`, **locally** with <a href=\"https://www.anaconda.com/distribution/\">Anaconda</a> or **online** on <a href=\"https://colab.research.google.com/\">Google Colab</a>\n",
        "        * Colab also supports running code on GPU, so if you don't have one, Colab is the way to go. To enable GPU on Colab, in the menu: `Runtime`$\\rightarrow$ `Change Runtime Type` $\\rightarrow$`GPU`.\n",
        "    2. Python IDE such as <a href=\"https://www.jetbrains.com/pycharm/\">PyCharm</a> or <a href=\"https://code.visualstudio.com/\">Visual Studio Code</a>.\n",
        "        * Both allow editing and running Jupyter Notebooks.\n",
        "\n",
        "* Please refer to `Setting Up the Working Environment.pdf` on the Moodle or our GitHub (https://github.com/taldatech/ee046211-deep-learning) to help you get everything installed.\n",
        "* If you need any technical assistance, please go to our Piazza forum (`hw3` folder) and describe your problem (preferably with images)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlp1Fp4ppGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
        "---\n",
        "* [Part 1 - Theory](#-Part-1---Theory)\n",
        "    * [Q1 - Masking in Transformers](#-Question-1---Masking-in-Transformers)\n",
        "    * [Q2 - Preventing Variance Explosion](#-Question-2---Preventing-Variance-Explosion)\n",
        "    * [Q3 - Recurrent Neural Networks](#-Question-3---Recurrent-Neural-Networks)\n",
        "* [Part 2 - Code Assignments - Sequence-to-Sequence with Transformers](#-Part-2---Code-Assignments)\n",
        "    * [Task 1 - Loading and Observing the Data](#-Task-1----Loading-and-Observing-the-Data)\n",
        "    * [Task 2 - Preparing the Data - Separating to Inputs and Targets](#-Task-2----Preparing-the--Data---Separating-to-Inputs-and-Targets)\n",
        "    * [Task 3 - Define Hyperparameters and Initialize the Model](#-Task-3----Define-Hyperparameters-and-Initialize-the-Model)\n",
        "    * [Task 4 - Train and Evaluate the Language Model](#-Task-4----Train-and-Evaluate-the-Language-Model)\n",
        "    * [Task 5 - Generate Sentences](#-Task-5----Generate-Sentences)\n",
        "* [Credits](#-Credits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKtSiQX_pGhj"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/ball-point-pen.png\" style=\"height:50px;display:inline\"> Part 1 - Theory\n",
        "---\n",
        "* You can choose whether to answser these straight in the notebook (Markdown + Latex) or use another editor (Word, LyX, Latex, Overleaf...) and submit an additional PDF file, **but no handwritten submissions**.\n",
        "* You can attach additional figures (drawings, graphs,...) in a separate PDF file, just make sure to refer to them in your answers.\n",
        "\n",
        "* $\\large\\LaTeX$ <a href=\"https://kapeli.com/cheat_sheets/LaTeX_Math_Symbols.docset/Contents/Resources/Documents/index\">Cheat-Sheet</a> (to write equations)\n",
        "    * <a href=\"http://tug.ctan.org/info/latex-refsheet/LaTeX_RefSheet.pdf\">Another Cheat-Sheet</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsouAL7mOrM6"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 1 - Masking in Transformers\n",
        "---\n",
        "We examine the classic encoder-decoder transformer model for next-token prediction, and focus on its masking mechanism.\n",
        "\n",
        "1. Draw a picture of the encoder-decoder transformer model, and qualitatively explain all its parts.\n",
        "2. Why do we need \"Masked self-attention\" in the decoder, instead of the regular self attention? Give an example with a specific task.\n",
        "3. To implement Multi-Head Masked Self-Attention (MHMSA) in the decoder we use\n",
        "   $$\\forall i\\in\\{1,\\ldots,H\\}:\\ Q_i=XW_i^Q,K_i=XW_i^K, V_i=XW_i^V$$\n",
        "   $$\\mathrm{MHMSA}(X)=\\sum_{i=1}^H \\mathrm{Softmax}\\left (\\frac{Q_i K_i^T}{\\sqrt{d}}+M\\right )V_i W_i^O$$\n",
        "   Explain how should we choose $M ∈ R^{T× T}$ to implement masking based on the expression for MHMSA above, and why this works.\n",
        "   \n",
        "4. Why do we not require masking in the Multi-Head Self-Attention (MHSA) right after the MHMSA? Base your answer on the mathematical expression for the MHSA.\n",
        "5. Explain the difference in operation of the MHMSA in training and inference. Which method is faster (assuming modern GPU hardware)?\n",
        "6. A researcher though it might be useful if (similarly to vision models) we add a \"max-pooling\" layer with pooling size 2 on the token dimension at the end of the first decoder blocks (after the fully connected layers), to reduce its size from $T$ to $T/2$, and then extend it back to the original $T$ (e.g., by replicating the tokens) before the last decoder block. Explain why is this problematic (hint: it is related to the reason we need masking), and how to fix the issue above, allowing this max-pooling layer to be added."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 1 - part 1\n",
        "---\n",
        "### <img src=\"https://www.researchgate.net/publication/363756886/figure/fig3/AS:11431281086057421@1664065203494/The-transformer-encoder-decoder-model.jpg\" style=\"height:50px;display:inline\">\n",
        "**Encoder** - Embeds the inputs and adds positional encodeing.\n",
        "\n",
        "Than computes self attention and uses normanliztion.\n",
        "\n",
        "Feed forward - takes all infromation and proceses it\n",
        "\n",
        "The output is a sequence of contextualized embeddings – one per token – which summarize the input.\n",
        "\n",
        "**Decoder** - The decoder begins by taking the output embeddings (usually shifted right during training) and adds positional encodings to retain sequence order.\n",
        "\n",
        "It first applies masked self-attention, allowing each position to attend only to earlier positions to prevent information leakage.\n",
        "\n",
        "Then, it performs encoder-decoder attention, where the queries come from the decoder and the keys and values come from the encoder’s output—this allows the decoder to align with the input context.\n",
        "\n",
        "Finally, it passes the result through a feed-forward network, identical in structure to the encoder’s, followed by normalization and residual connections.\n",
        "\n",
        "Part 2\n",
        "---\n",
        "During training, we use masked self attention to prevent the decoder to look on future tookens. This is why we shift the tokens.\n",
        "\n",
        "\n",
        "example:\n",
        "\n",
        "Suppose we're translating from French to English:\n",
        "\n",
        "**Input**: “Je suis étudiant”\n",
        "\n",
        "**Target**: “I am a student”\n",
        "\n",
        "At step 2 we want the model to predict am if it has I. without masking it would see \"student\" beforehand.\n",
        "\n",
        "Part 3\n",
        "---\n",
        "We will define M as\n",
        "$\n",
        "M_{ij} =\n",
        "\\begin{cases}\n",
        "0 & \\text{if } j \\leq i \\\\\n",
        "-\\infty & \\text{if } j > i\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "In this way when we have a future token $(j>i)$ the probability will be zero O\n",
        "(due to softmax), and in other cases it will behave as a normal softmax, allowing attention to the current and previous tokens only.\n",
        "Part 4\n",
        "---\n",
        "We do not require mask after the MHMSA because the encoder proccesses the whole input simulataniosly.\n",
        "\n",
        "As we can see in the equation below - there is no M:\n",
        "\n",
        "   $$\\mathrm{MHSA}(X)=\\sum_{i=1}^H \\mathrm{Softmax}\\left (\\frac{Q_i K_i^T}{\\sqrt{d}}\\right )V_i W_i^O$$\n",
        "\n",
        "Part 5\n",
        "---\n",
        "During inference - we can't use full matrix operaation because future tokens don't exist yet! The decoding is sequential. It's much slower due to parallelization at all times which can be done at training.\n",
        "\n",
        "Part 6\n",
        "---\n",
        "If we use max-pooling as  suggested we can take token at time $t$ and $t+1$ and pool them together, thus allowing access to future tokens!\n",
        "\n",
        "We can use causal max pooling as a fix and mix tokens with only previous ones at each time."
      ],
      "metadata": {
        "id": "FCi5xloBQSei"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAn4joZ1QjU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsqSFZG1pGhj"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 2 - Preventing Variance Explosion\n",
        "---\n",
        "This question relates to lectures 8-9 (from slide 7):\n",
        "\n",
        "Find an initializtion scheme such that $$ \\forall l, i,: \\text{(1) } \\mathbb{E}\\left[F_l(u_l)|u_l\\right]=0, \\text{ (2) } Var(u_l[i]) = 1, $$ assuming skip connections: $u_{l+1} = u_l + F_l(u_l)$ with a single skip $F_l(u_l)=W_l\\phi(u_l)+b_l$ and the activation is ReLU: $\\phi(x) = \\text{ReLU}(x) = \\max(0,x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 2\n",
        "---\n",
        "\n",
        "$F_l(u_l) = W_l \\cdot \\text{ReLU}(u_l) + b_l$\n",
        "$$\n",
        "\\mathbb{E}[F_l(u_l)] = \\mathbb{E}[W_l] \\cdot \\text{ReLU}(u_l) + \\mathbb{E}[b_l]\n",
        "$$\n",
        "We need:\n",
        "\n",
        "$\\mathbb{E}[F_l(u_l)] = 0$\n",
        "$$\n",
        "\\mathbb{E}[W_l] = 0, \\quad \\mathbb{E}[b_l] = 0\n",
        "$$\n",
        "Will work! We will take EW=Eb=0 and add small noise (variance).\n",
        "\n",
        "Lets check for the var, for all i\n",
        "\n",
        "$$\n",
        "u_l = u_{l-1} + F_{l-1}(u_{l-1}), \\quad F_{l-1}(u_{l-1}) = W_{l-1} \\cdot \\phi(u_{l-1}) + b_{l-1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Var}(u_l[i]) = \\mathbb{E}[\\text{Var}(F_{l-1}(u_{l-1}[i]) \\mid u_{l-1})] + \\text{Var}(u_{l-1}[i])\n",
        "$$\n",
        "\n",
        "And thus:\n",
        "\n",
        "$$\n",
        "\\text{Var}(u_l[i]) \\approx \\text{Var}(u_{l-1}[i]) = 1\n",
        "$$ for all i\n",
        "\n"
      ],
      "metadata": {
        "id": "lIQvim2CavWm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czyps431OrM6"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/clouds/100/000000/question-mark.png\" style=\"height:50px;display:inline\"> Question 3 - Recurrent Neural Networks\n",
        "---\n",
        "You are given a recurrent/feedback neural network with LReLU activations $\\phi(u) = \\max[pu,u]$, with input $x_t$ and a representation $v_t \\in \\mathbb{R}^d$ that is updated as follows:\n",
        "$$ \\forall \\tau=1,2,...t\\:: v_{\\tau}=\\phi(u_{\\tau})\\:, u_{\\tau} = Wv_{\\tau - 1} + B x_{\\tau},$$\n",
        "from initialization $v_0$, and outputs $\\hat{y}_t=Cv_t$. The network is trained with GD on a single long series $\\{x_{\\tau}, y_{\\tau}\\}_{\\tau=1}^t$ with a cost function $\\ell(y_t, \\hat{y}_t)$ over the last term in the series.\n",
        "\n",
        "1. Calculate the exact gradient $\\frac{\\partial \\ell}{\\partial W[i,j]}$ using Backpropagation through time (BPTT).\n",
        "2. Recall that calculating the gradient using the method in the previous section there are two issues for $t \\to \\infty$: (1) the required computational resources grow indefinitely, and (2) the gradients explode or vanish. For each problem: explain it, provide an example for a method to alleviate it and describe any limitations of this method."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1\n",
        "---\n",
        "$$\n",
        "\\frac{\\partial \\ell(y_t, \\hat{y}_t)}{\\partial W[i,j]} =\n",
        "\\frac{\\partial \\ell}{\\partial \\hat{y}_t}\n",
        "\\cdot \\frac{\\partial \\hat{y}_t}{\\partial v_t}\n",
        "\\cdot \\sum_{t'=1}^{t}\n",
        "\\left(\n",
        "\\prod_{\\tau = t'+1}^{t}\n",
        "\\frac{\\partial v_\\tau}{\\partial v_{\\tau-1}}\n",
        "\\right)\n",
        "\\cdot \\frac{\\partial v_{t'}}{\\partial u_{t'}[i]} \\cdot v_{t'-1}[j]\n",
        "=\n",
        "\\left[\n",
        "\\frac{\\partial \\ell}{\\partial \\hat{y}_t}\n",
        "\\cdot C\n",
        "\\cdot \\sum_{t'=1}^{t}\n",
        "\\left(\n",
        "\\prod_{\\tau = t'+1}^{t}\n",
        "\\text{Diag}(\\phi'(u_\\tau)) \\cdot W\n",
        "\\right)\n",
        "\\cdot \\text{Diag}(\\phi'(u_{t'}))\n",
        "\\right]_{i}\n",
        "\\cdot v_{t'-1}[j]\n",
        "$$\n",
        "\n",
        "and in other way:\n",
        "defining\n",
        "\n",
        "$$\n",
        "\\bar{v}_t = C^T \\cdot \\frac{\\partial \\ell}{\\partial \\hat{y}_t}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\bar{v}_{\\tau - 1} = W^T \\cdot \\text{Diag}(\\phi'(u_\\tau)) \\cdot \\bar{v}_\\tau\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell(y_t, \\hat{y}_t)}{\\partial W} =\n",
        "\\sum_{t'=1}^t \\text{Diag}(\\phi'(u_{t'})) \\cdot \\bar{v}_{t'} \\cdot v_{t'-1}^T\n",
        "$$\n",
        "\n",
        "Part 2\n",
        "---\n",
        "In forward pass we need to store the values of $\\bar{v}_t, v_t,u_t$ which is hard for big ts. Can be fixed by going only number of steps back each time (fixed number)  **Truncated Backpropagation Through Time (TBPTT)** . This reduces computational resources but sacrifices accuracy.\n",
        "\n",
        "The second problem can be solves with gradient-clipping or using gating mecanismes as GRU and LSTM"
      ],
      "metadata": {
        "id": "cJfqU8LngrCe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D-14iM7pGhm"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/officel/80/000000/code.png\" style=\"height:50px;display:inline\"> Part 2 - Code Assignments\n",
        "---\n",
        "* You must write your code in this notebook and save it with the output of all of the code cells.\n",
        "* Additional text can be added in Markdown cells.\n",
        "* You can use any other IDE you like (PyCharm, VSCode...) to write/debug your code, but for the submission you must copy it to this notebook, run the code and save the notebook with the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljG3GY1kOrM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ae6281-bdcd-4246-b04c-f3a095a37969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata) (2.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2->torchdata)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-3.1.1\n"
          ]
        }
      ],
      "source": [
        "# this part uses the Wikitext-2 dataset. To access torchtext datasets, please install `torchdata`:\n",
        "# `pip install torchdata` ir `conda install -c pytorch torchdata` in activated environment\n",
        "# or `!pip install torchdata` on colab.\n",
        "!pip install torchdata\n",
        "# notes:\n",
        "# torch=2.0.0 <-> torchtext 0.15.1\n",
        "# torch=1.13.0 <-> torchtext 0.14.0\n",
        "# torch=1.12.1 <-> torchtext 0.13.1\n",
        "# downgrading torchtext example: !pip install torchtext==0.13.1 --no-deps\n",
        "# torchtext requires the `portalocker` package to download datasets:\n",
        "!pip install portalocker"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.2.0 torchtext==0.17.0 --force-reinstall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pdeVUDTvaiAe",
        "outputId": "03b807bf-857a-4f3c-fb0e-ad403b40bbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.0\n",
            "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchtext==0.17.0\n",
            "  Downloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting filelock (from torch==2.2.0)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.2.0)\n",
            "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.2.0)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.2.0)\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch==2.2.0)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch==2.2.0)\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting tqdm (from torchtext==0.17.0)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests (from torchtext==0.17.0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting numpy (from torchtext==0.17.0)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting urllib3>=1.25 (from torchdata==0.7.1->torchtext==0.17.0)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.2.0)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->torchtext==0.17.0)\n",
            "  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torchtext==0.17.0)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torchtext==0.17.0)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.0)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchdata, torchtext\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.2\n",
            "    Uninstalling charset-normalizer-3.4.2:\n",
            "      Successfully uninstalled charset-normalizer-3.4.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.4.26\n",
            "    Uninstalling certifi-2025.4.26:\n",
            "      Successfully uninstalled certifi-2025.4.26\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.11.0\n",
            "    Uninstalling torchdata-0.11.0:\n",
            "      Successfully uninstalled torchdata-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 certifi-2025.4.26 charset-normalizer-3.4.2 filelock-3.18.0 fsspec-2025.5.1 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.2.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 requests-2.32.3 sympy-1.14.0 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 tqdm-4.67.1 triton-2.2.0 typing-extensions-4.14.0 urllib3-2.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              },
              "id": "76d38d415c5e49cdaa88b2311c86e3f8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3eyvdl6OrM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbed317-d6f8-4199-90fe-084b3b035475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-f263d2332cdd>\", line 10, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1471, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7852b423deb0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# imports for the practice (you can add more if you need)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "# torchtext\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "seed = 211\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG-IqzpUOrM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32aea1ad-2046-469f-bf59-0a7748f475b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch: 2.2.0+cu121, torchtext: 0.17.0+cpu\n"
          ]
        }
      ],
      "source": [
        "print(f'pytorch: {torch.__version__}, torchtext: {torchtext.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4bCPSL-OrM7"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/bubbles/50/000000/workflow.png\" style=\"height:50px;display:inline\">  Sequence-to-Sequence with Transformers\n",
        "---\n",
        "* In this exercise, you are going to build a language model using PyTroch's Transformer module.\n",
        "* We will work with the **Wikitext-2** dataset: the WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia.\n",
        "* After training, you will be able to generate senetences!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntF_dABOOrM7"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 1  - Loading and Observing the Data\n",
        "---\n",
        "1. Run the following cells that define the functions `batchify` and `data_process` and initialize the tokenizer, vocabulary and the WikiText2 train dataset.\n",
        "2. Create the train, valid and test data using the provided `batchify` function.\n",
        "5. Print the shape of `train_data`, write in a comment the meaning of each dimension (e.g. `# [meaning of dim1, meaning of dim2]`).\n",
        "6. Print the first 20 words of one training sample from `train_data`. Use the vocabulary you built to transfer between tokens to words: `itos = vocab.vocab.get_itos()` will give a \"int to string\" list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU6Wsni_OrM7"
      },
      "outputs": [],
      "source": [
        "def batchify(data, bsz):\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2fqLfaqOrM7"
      },
      "outputs": [],
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOjjWaq9OrM7"
      },
      "outputs": [],
      "source": [
        "with open(\"data/wiki.train.tokens\", encoding=\"utf-8\") as f:\n",
        "    train_iter = [line.strip() for line in f if line.strip() and not line.startswith('=')]\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-msaEe7SOrM8"
      },
      "outputs": [],
      "source": [
        "def load_wikitext_split(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return [line.strip() for line in f if line.strip() and not line.startswith('=')]\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "\n",
        "train_iter = load_wikitext_split(\"data/wiki.train.tokens\")\n",
        "val_iter   = load_wikitext_split(\"data/wiki.valid.tokens\")\n",
        "test_iter  = load_wikitext_split(\"data/wiki.test.tokens\")\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(val_data), len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZXyy009kQqc",
        "outputId": "511b9269-fd56-472b-830a-11b3b7209a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2049990 214417 241859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOuUY6DbOrM8"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "eval_batch_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzdkLqhsOrM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081de758-3734-4560-dc03-6a2e08e0a145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([102499, 20])\n",
            "torch.Size([21441, 10])\n",
            "torch.Size([24185, 10])\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\"\n",
        "train_data_b = batchify(train_data,batch_size)\n",
        "val_data_b = batchify(val_data,eval_batch_size)\n",
        "test_data_b = batchify(test_data,eval_batch_size)\n",
        "\n",
        "print(train_data_b.shape)\n",
        "print(val_data_b.shape)\n",
        "print(test_data_b.shape)\n",
        "\n",
        "# Each of the tensor's size is the len/batch_size\n",
        "#[ 2049990/1= =102499, 20 batches]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "itos = vocab.vocab.get_itos()\n",
        "first_sample_tokens = train_data_b[:, 0][:20] # Take the first column (sample) and first 20 rows (tokens)\n",
        "first_sample_words = [itos[token] for token in first_sample_tokens]\n",
        "print(first_sample_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpDprX7FmYfn",
        "outputId": "5935ef8f-2210-4a83-8dcc-a8e102c86021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['=', 'valkyria', 'chronicles', 'iii', '=', 'senjō', 'no', 'valkyria', '3', '<unk>', 'chronicles', '(', 'japanese', '戦場のヴァルキュリア3', ',', 'lit', '.', 'valkyria', 'of', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y7UAHRKOrM8"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 2  - Preparing the  Data - Separating to Inputs and Targets\n",
        "---\n",
        "* For a language modeling task, the model needs the following words as `Target`.\n",
        "    * For example, for the senetence \"I have a nice dog\", the model will be given \"I have a nice\" as input, and \"have a nice dog\" as the target.\n",
        "* Implement (complete) the function `get_batch(source, i, bptt)`: it generates the input and target sequence for the transformer model. It subdivides the source data into chunks of length `bptt`.\n",
        "    * For example, for `bptt=2` and at `i=0`, the output of `data, target = get_batch(train_data, i=0, bptt=2)`: `data` will be of shape (2, 20), where the batch size is 20 and `target` will be of length 40 (the target for each element is two words, but we flatten `target`).\n",
        "    * Example: for `bptt=2`, and the ABCDEFG... characters as input, our batches will be in the form of: `data=[a, b], target=[b, c]`. For `bptt=3`: `data=[a, b, c], target=[b, c, d]` and so on. This one example is a batch.\n",
        "    * Print a sample from `data` and `target`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEZrRxCcOrM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323e5248-bdbb-4678-f9b0-d070c009ae9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using bptt = 5 and batch size = 20\n",
            "Data Shape: torch.Size([5])\n",
            "Target Shape: torch.Size([5])\n",
            "Data is [ = valkyria chronicles iii = ]\n",
            "Target is [ valkyria chronicles iii = senjō ]\n"
          ]
        }
      ],
      "source": [
        "def get_batch(source, i, bptt):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "        bptt: int\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i + seq_len]\n",
        "    target = source[i + 1:i + 1 + seq_len].reshape(-1)\n",
        "    return data, target\n",
        "\n",
        "sample_bptt = 5\n",
        "sample_batch = get_batch(train_data, 0, sample_bptt)\n",
        "sample_data = ''\n",
        "sample_target = ''\n",
        "print(f'Using bptt = {sample_bptt} and batch size = {batch_size}')\n",
        "print(f'Data Shape: {sample_batch[0].shape}')\n",
        "print(f'Target Shape: {sample_batch[1].shape}')\n",
        "\n",
        "for i in range(sample_bptt):\n",
        "    sample_data_code = sample_batch[0][i]\n",
        "    sample_target_code = sample_batch[1][i]\n",
        "    sample_data += f'{itos[sample_data_code]} '\n",
        "    sample_target += f'{itos[sample_target_code]} '\n",
        "print(f'Data is [ {sample_data}]')\n",
        "print(f'Target is [ {sample_target}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ee536k_OrM8"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 3  - Define Hyperparameters and Initialize the Model\n",
        "---\n",
        "* Define the following hyperparameters (`[a, b]` means in the range between `a` and `b`):\n",
        "    * Embedding size: choose from `[200, 250]`\n",
        "    * Number of hidden units: choose from `[200, 250]`\n",
        "    * Number of layers: choose from `[2, 4]`\n",
        "    * Number of attention heads: choose from `[2, 4]`\n",
        "    * Dropout: choose from `[0.0, 0.3]`\n",
        "    * Loss criterion: `nn.CrossEntropyLoss()`\n",
        "    * Optimizer: choose from `[SGD, Adam, RAdam]`\n",
        "    * Learning rate: choose from `[5e-3, 5.0]`\n",
        "    * Learning Scheduler: `torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)` or any scheduler of your choosing.\n",
        "    * Transformer LayerNormalization: `post` (`norm_first=False`) or `pre` (`norm_first=True`).\n",
        "* Intialize an instance of `TransformerModel` (given) and send it to `device`. Note that you need to give it the number of tokens to define the output of the decoder. You should use the number of tokens in the vocabulary. Print the number of tokens,  print **all** the chosen hyper-parameters and print the model (`print(model`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONmYYqmVOrM8"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, norm_first=False):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, norm_first=norm_first)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)\n",
        "\n",
        "# Hyperparameters\n",
        "emsize = 240  # embedding dimension\n",
        "d_hid = 250  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 3  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 3 # number of attention heads\n",
        "dropout = 0.3  # dropout probability\n",
        "lr = 0.01  # learning rate\n",
        "norm_first = False # post or pre layer normalization\n",
        "\n",
        "# Model initialization\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout, norm_first).to(device)\n",
        "pad_idx = vocab['<pad>']\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "\n",
        "print(f'Number of tokens: {ntokens}')\n",
        "print('Chosen Hyperparameters:')\n",
        "print(f'  Embedding size: {emsize}')\n",
        "print(f'  Number of hidden units: {d_hid}')\n",
        "print(f'  Number of layers: {nlayers}')\n",
        "print(f'  Number of attention heads: {nhead}')\n",
        "print(f'  Dropout: {dropout}')\n",
        "print(f'  Loss criterion: {criterion}')\n",
        "print(f'  Optimizer: {optimizer.__class__.__name__}')\n",
        "print(f'  Learning rate: {lr}')\n",
        "print(f'  Learning Scheduler: {scheduler.__class__.__name__}')\n",
        "print(f'  Transformer LayerNormalization: {\"pre\" if norm_first else \"post\"}')\n",
        "\n",
        "print('\\nModel Architecture:')\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWF9-5wNyf9x",
        "outputId": "e2e315bc-6a72-4a7e-ca2f-96b3de103370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 25703\n",
            "Chosen Hyperparameters:\n",
            "  Embedding size: 240\n",
            "  Number of hidden units: 250\n",
            "  Number of layers: 3\n",
            "  Number of attention heads: 3\n",
            "  Dropout: 0.3\n",
            "  Loss criterion: CrossEntropyLoss()\n",
            "  Optimizer: SGD\n",
            "  Learning rate: 0.01\n",
            "  Learning Scheduler: StepLR\n",
            "  Transformer LayerNormalization: post\n",
            "\n",
            "Model Architecture:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=240, out_features=240, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=240, out_features=250, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=250, out_features=240, bias=True)\n",
              "        (norm1): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((240,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (encoder): Embedding(25703, 240)\n",
              "  (decoder): Linear(in_features=240, out_features=25703, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOu8ZhszOrM8"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 4  - Train and Evaluate the Language Model\n",
        "---\n",
        "* Fill in the missing line in the training code and train the model.\n",
        "* Use `bptt=35`.\n",
        "* Use the provided function to evaluate it on the validatation set (after each epoch) and on test test (after training is done). **Print and plot** the results (loss and perplexity).\n",
        "* If you see that the performance does not improve, go back to Task 3 and re-think you hyper-parameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RmoWgW5k6T93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9PU3X4tOrM8"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, eval_data):\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i, bptt)\n",
        "            seq_len = data.size(0)\n",
        "            if seq_len != bptt:\n",
        "                src_mask = src_mask[:seq_len, :seq_len]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ0oz_iDOrM8"
      },
      "outputs": [],
      "source": [
        "def train(model, bptt):\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.     # total over the entire epoch\n",
        "    log_loss = 0.       # loss just for logging interval\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data_b) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data_b.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data_b, i, bptt)\n",
        "        seq_len = data.size(0)\n",
        "        if seq_len != bptt:\n",
        "            src_mask = model.generate_square_subsequent_mask(seq_len).to(device)\n",
        "\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = log_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            log_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_loss / num_batches\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "eRBPVYIf3dBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQqta15W3iLW",
        "outputId": "777fafa9-7158-445e-a66f-40e32ae96f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yWaxLIKOrM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6c816f0-f10b-437b-e258-45aa14fb4963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2928 batches | lr 0.01 | ms/batch 16.26 | loss  9.92 | ppl 20334.83\n",
            "| epoch   1 |   400/ 2928 batches | lr 0.01 | ms/batch 15.58 | loss  8.73 | ppl  6202.82\n",
            "| epoch   1 |   600/ 2928 batches | lr 0.01 | ms/batch 15.78 | loss  8.29 | ppl  3974.58\n",
            "| epoch   1 |   800/ 2928 batches | lr 0.01 | ms/batch 16.22 | loss  8.10 | ppl  3292.52\n",
            "| epoch   1 |  1000/ 2928 batches | lr 0.01 | ms/batch 16.08 | loss  7.93 | ppl  2778.70\n",
            "| epoch   1 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.83 | loss  7.85 | ppl  2561.62\n",
            "| epoch   1 |  1400/ 2928 batches | lr 0.01 | ms/batch 16.02 | loss  7.76 | ppl  2333.93\n",
            "| epoch   1 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.56 | loss  7.70 | ppl  2215.15\n",
            "| epoch   1 |  1800/ 2928 batches | lr 0.01 | ms/batch 16.23 | loss  7.68 | ppl  2154.12\n",
            "| epoch   1 |  2000/ 2928 batches | lr 0.01 | ms/batch 16.12 | loss  7.65 | ppl  2098.23\n",
            "| epoch   1 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.08 | loss  7.53 | ppl  1868.58\n",
            "| epoch   1 |  2400/ 2928 batches | lr 0.01 | ms/batch 16.63 | loss  7.51 | ppl  1817.70\n",
            "| epoch   1 |  2600/ 2928 batches | lr 0.01 | ms/batch 16.09 | loss  7.46 | ppl  1739.61\n",
            "| epoch   1 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.96 | loss  7.46 | ppl  1730.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 49.60s | valid loss  7.31 | valid ppl  1502.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2928 batches | lr 0.01 | ms/batch 16.05 | loss  7.44 | ppl  1705.72\n",
            "| epoch   2 |   400/ 2928 batches | lr 0.01 | ms/batch 15.87 | loss  7.37 | ppl  1583.02\n",
            "| epoch   2 |   600/ 2928 batches | lr 0.01 | ms/batch 15.81 | loss  7.35 | ppl  1550.15\n",
            "| epoch   2 |   800/ 2928 batches | lr 0.01 | ms/batch 16.15 | loss  7.36 | ppl  1566.24\n",
            "| epoch   2 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  7.33 | ppl  1526.83\n",
            "| epoch   2 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  7.33 | ppl  1520.66\n",
            "| epoch   2 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.78 | loss  7.29 | ppl  1469.07\n",
            "| epoch   2 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.10 | loss  7.28 | ppl  1445.20\n",
            "| epoch   2 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.74 | loss  7.27 | ppl  1435.06\n",
            "| epoch   2 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.74 | loss  7.27 | ppl  1431.37\n",
            "| epoch   2 |  2200/ 2928 batches | lr 0.01 | ms/batch 15.78 | loss  7.17 | ppl  1305.26\n",
            "| epoch   2 |  2400/ 2928 batches | lr 0.01 | ms/batch 16.42 | loss  7.17 | ppl  1303.40\n",
            "| epoch   2 |  2600/ 2928 batches | lr 0.01 | ms/batch 16.01 | loss  7.16 | ppl  1284.68\n",
            "| epoch   2 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.87 | loss  7.16 | ppl  1287.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 49.04s | valid loss  7.03 | valid ppl  1127.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2928 batches | lr 0.01 | ms/batch 15.98 | loss  7.17 | ppl  1302.58\n",
            "| epoch   3 |   400/ 2928 batches | lr 0.01 | ms/batch 15.81 | loss  7.12 | ppl  1238.32\n",
            "| epoch   3 |   600/ 2928 batches | lr 0.01 | ms/batch 15.80 | loss  7.10 | ppl  1216.16\n",
            "| epoch   3 |   800/ 2928 batches | lr 0.01 | ms/batch 16.31 | loss  7.12 | ppl  1242.64\n",
            "| epoch   3 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.78 | loss  7.10 | ppl  1216.08\n",
            "| epoch   3 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.74 | loss  7.12 | ppl  1235.44\n",
            "| epoch   3 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.91 | loss  7.10 | ppl  1209.61\n",
            "| epoch   3 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.36 | loss  7.09 | ppl  1203.39\n",
            "| epoch   3 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.84 | loss  7.08 | ppl  1191.38\n",
            "| epoch   3 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.81 | loss  7.09 | ppl  1198.27\n",
            "| epoch   3 |  2200/ 2928 batches | lr 0.01 | ms/batch 15.87 | loss  7.00 | ppl  1102.11\n",
            "| epoch   3 |  2400/ 2928 batches | lr 0.01 | ms/batch 16.16 | loss  7.01 | ppl  1110.76\n",
            "| epoch   3 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.92 | loss  7.01 | ppl  1103.29\n",
            "| epoch   3 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.78 | loss  7.00 | ppl  1101.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 49.23s | valid loss  6.88 | valid ppl   969.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2928 batches | lr 0.01 | ms/batch 15.82 | loss  7.03 | ppl  1131.29\n",
            "| epoch   4 |   400/ 2928 batches | lr 0.01 | ms/batch 15.70 | loss  6.99 | ppl  1087.54\n",
            "| epoch   4 |   600/ 2928 batches | lr 0.01 | ms/batch 15.64 | loss  6.97 | ppl  1061.04\n",
            "| epoch   4 |   800/ 2928 batches | lr 0.01 | ms/batch 16.30 | loss  6.99 | ppl  1090.52\n",
            "| epoch   4 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.81 | loss  6.97 | ppl  1067.76\n",
            "| epoch   4 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.86 | loss  7.00 | ppl  1093.69\n",
            "| epoch   4 |  1400/ 2928 batches | lr 0.01 | ms/batch 16.14 | loss  6.98 | ppl  1075.17\n",
            "| epoch   4 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.16 | loss  6.98 | ppl  1074.08\n",
            "| epoch   4 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.73 | loss  6.97 | ppl  1062.01\n",
            "| epoch   4 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.74 | loss  6.98 | ppl  1076.15\n",
            "| epoch   4 |  2200/ 2928 batches | lr 0.01 | ms/batch 15.87 | loss  6.90 | ppl   991.33\n",
            "| epoch   4 |  2400/ 2928 batches | lr 0.01 | ms/batch 16.06 | loss  6.91 | ppl  1004.56\n",
            "| epoch   4 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.73 | loss  6.91 | ppl  1000.40\n",
            "| epoch   4 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.73 | loss  6.91 | ppl  1001.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 49.06s | valid loss  6.78 | valid ppl   879.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2928 batches | lr 0.01 | ms/batch 15.77 | loss  6.94 | ppl  1032.83\n",
            "| epoch   5 |   400/ 2928 batches | lr 0.01 | ms/batch 15.70 | loss  6.91 | ppl   998.65\n",
            "| epoch   5 |   600/ 2928 batches | lr 0.01 | ms/batch 15.72 | loss  6.88 | ppl   970.23\n",
            "| epoch   5 |   800/ 2928 batches | lr 0.01 | ms/batch 16.34 | loss  6.91 | ppl  1000.03\n",
            "| epoch   5 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.75 | loss  6.89 | ppl   980.07\n",
            "| epoch   5 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  6.92 | ppl  1009.29\n",
            "| epoch   5 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.74 | loss  6.90 | ppl   994.24\n",
            "| epoch   5 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.31 | loss  6.91 | ppl   999.05\n",
            "| epoch   5 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.85 | loss  6.89 | ppl   982.04\n",
            "| epoch   5 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.73 | loss  6.91 | ppl   997.46\n",
            "| epoch   5 |  2200/ 2928 batches | lr 0.01 | ms/batch 15.89 | loss  6.83 | ppl   921.26\n",
            "| epoch   5 |  2400/ 2928 batches | lr 0.01 | ms/batch 16.02 | loss  6.84 | ppl   936.37\n",
            "| epoch   5 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.66 | loss  6.84 | ppl   935.75\n",
            "| epoch   5 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  6.84 | ppl   934.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 48.91s | valid loss  6.71 | valid ppl   822.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2928 batches | lr 0.01 | ms/batch 15.88 | loss  6.88 | ppl   968.26\n",
            "| epoch   6 |   400/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.84 | ppl   937.03\n",
            "| epoch   6 |   600/ 2928 batches | lr 0.01 | ms/batch 15.86 | loss  6.81 | ppl   908.17\n",
            "| epoch   6 |   800/ 2928 batches | lr 0.01 | ms/batch 16.25 | loss  6.85 | ppl   942.80\n",
            "| epoch   6 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.76 | loss  6.83 | ppl   920.97\n",
            "| epoch   6 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.65 | loss  6.86 | ppl   952.17\n",
            "| epoch   6 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.94 | loss  6.85 | ppl   940.12\n",
            "| epoch   6 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.03 | loss  6.85 | ppl   943.39\n",
            "| epoch   6 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.66 | loss  6.83 | ppl   926.28\n",
            "| epoch   6 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.85 | loss  6.85 | ppl   945.88\n",
            "| epoch   6 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.05 | loss  6.77 | ppl   871.91\n",
            "| epoch   6 |  2400/ 2928 batches | lr 0.01 | ms/batch 16.11 | loss  6.79 | ppl   887.60\n",
            "| epoch   6 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.79 | ppl   888.83\n",
            "| epoch   6 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.79 | ppl   889.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 48.96s | valid loss  6.66 | valid ppl   780.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2928 batches | lr 0.01 | ms/batch 15.75 | loss  6.83 | ppl   920.94\n",
            "| epoch   7 |   400/ 2928 batches | lr 0.01 | ms/batch 15.68 | loss  6.80 | ppl   894.98\n",
            "| epoch   7 |   600/ 2928 batches | lr 0.01 | ms/batch 15.89 | loss  6.76 | ppl   866.43\n",
            "| epoch   7 |   800/ 2928 batches | lr 0.01 | ms/batch 16.18 | loss  6.80 | ppl   899.40\n",
            "| epoch   7 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  6.78 | ppl   876.22\n",
            "| epoch   7 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.75 | loss  6.81 | ppl   906.81\n",
            "| epoch   7 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.86 | loss  6.80 | ppl   900.29\n",
            "| epoch   7 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.12 | loss  6.81 | ppl   903.45\n",
            "| epoch   7 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.74 | loss  6.79 | ppl   885.74\n",
            "| epoch   7 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.82 | loss  6.81 | ppl   903.21\n",
            "| epoch   7 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.05 | loss  6.73 | ppl   833.94\n",
            "| epoch   7 |  2400/ 2928 batches | lr 0.01 | ms/batch 16.02 | loss  6.75 | ppl   852.44\n",
            "| epoch   7 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.88 | loss  6.75 | ppl   853.91\n",
            "| epoch   7 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.75 | ppl   851.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 49.04s | valid loss  6.62 | valid ppl   746.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2928 batches | lr 0.01 | ms/batch 15.83 | loss  6.79 | ppl   884.56\n",
            "| epoch   8 |   400/ 2928 batches | lr 0.01 | ms/batch 15.84 | loss  6.76 | ppl   861.78\n",
            "| epoch   8 |   600/ 2928 batches | lr 0.01 | ms/batch 15.92 | loss  6.72 | ppl   832.52\n",
            "| epoch   8 |   800/ 2928 batches | lr 0.01 | ms/batch 16.37 | loss  6.76 | ppl   864.41\n",
            "| epoch   8 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.77 | loss  6.74 | ppl   842.99\n",
            "| epoch   8 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.83 | loss  6.77 | ppl   872.56\n",
            "| epoch   8 |  1400/ 2928 batches | lr 0.01 | ms/batch 16.05 | loss  6.76 | ppl   863.68\n",
            "| epoch   8 |  1600/ 2928 batches | lr 0.01 | ms/batch 15.93 | loss  6.77 | ppl   871.14\n",
            "| epoch   8 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.82 | loss  6.75 | ppl   854.79\n",
            "| epoch   8 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.77 | ppl   869.67\n",
            "| epoch   8 |  2200/ 2928 batches | lr 0.01 | ms/batch 15.94 | loss  6.69 | ppl   801.93\n",
            "| epoch   8 |  2400/ 2928 batches | lr 0.01 | ms/batch 15.99 | loss  6.71 | ppl   822.46\n",
            "| epoch   8 |  2600/ 2928 batches | lr 0.01 | ms/batch 16.26 | loss  6.71 | ppl   824.52\n",
            "| epoch   8 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  6.71 | ppl   821.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 49.00s | valid loss  6.58 | valid ppl   720.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2928 batches | lr 0.01 | ms/batch 15.84 | loss  6.75 | ppl   854.61\n",
            "| epoch   9 |   400/ 2928 batches | lr 0.01 | ms/batch 15.73 | loss  6.73 | ppl   833.59\n",
            "| epoch   9 |   600/ 2928 batches | lr 0.01 | ms/batch 15.87 | loss  6.69 | ppl   802.50\n",
            "| epoch   9 |   800/ 2928 batches | lr 0.01 | ms/batch 16.23 | loss  6.73 | ppl   836.22\n",
            "| epoch   9 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.65 | loss  6.70 | ppl   813.19\n",
            "| epoch   9 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.77 | loss  6.74 | ppl   844.64\n",
            "| epoch   9 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.99 | loss  6.73 | ppl   840.13\n",
            "| epoch   9 |  1600/ 2928 batches | lr 0.01 | ms/batch 15.89 | loss  6.74 | ppl   843.78\n",
            "| epoch   9 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.76 | loss  6.72 | ppl   826.95\n",
            "| epoch   9 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.77 | loss  6.74 | ppl   843.97\n",
            "| epoch   9 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.13 | loss  6.66 | ppl   777.51\n",
            "| epoch   9 |  2400/ 2928 batches | lr 0.01 | ms/batch 15.99 | loss  6.68 | ppl   799.88\n",
            "| epoch   9 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.67 | loss  6.68 | ppl   799.80\n",
            "| epoch   9 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.69 | loss  6.68 | ppl   799.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 48.84s | valid loss  6.55 | valid ppl   698.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.72 | ppl   830.71\n",
            "| epoch  10 |   400/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  6.70 | ppl   810.70\n",
            "| epoch  10 |   600/ 2928 batches | lr 0.01 | ms/batch 15.93 | loss  6.66 | ppl   781.00\n",
            "| epoch  10 |   800/ 2928 batches | lr 0.01 | ms/batch 16.07 | loss  6.70 | ppl   812.11\n",
            "| epoch  10 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.70 | loss  6.67 | ppl   790.43\n",
            "| epoch  10 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.72 | loss  6.71 | ppl   822.44\n",
            "| epoch  10 |  1400/ 2928 batches | lr 0.01 | ms/batch 16.13 | loss  6.70 | ppl   814.44\n",
            "| epoch  10 |  1600/ 2928 batches | lr 0.01 | ms/batch 15.83 | loss  6.71 | ppl   820.61\n",
            "| epoch  10 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  6.69 | ppl   806.12\n",
            "| epoch  10 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.68 | loss  6.71 | ppl   821.60\n",
            "| epoch  10 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.07 | loss  6.63 | ppl   757.43\n",
            "| epoch  10 |  2400/ 2928 batches | lr 0.01 | ms/batch 15.94 | loss  6.66 | ppl   778.46\n",
            "| epoch  10 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.66 | loss  6.66 | ppl   780.20\n",
            "| epoch  10 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.66 | loss  6.66 | ppl   777.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 48.77s | valid loss  6.52 | valid ppl   679.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 2928 batches | lr 0.01 | ms/batch 15.84 | loss  6.70 | ppl   809.57\n",
            "| epoch  11 |   400/ 2928 batches | lr 0.01 | ms/batch 15.75 | loss  6.67 | ppl   791.10\n",
            "| epoch  11 |   600/ 2928 batches | lr 0.01 | ms/batch 15.84 | loss  6.63 | ppl   759.92\n",
            "| epoch  11 |   800/ 2928 batches | lr 0.01 | ms/batch 16.10 | loss  6.68 | ppl   792.76\n",
            "| epoch  11 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.70 | loss  6.65 | ppl   770.94\n",
            "| epoch  11 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.78 | loss  6.69 | ppl   801.52\n",
            "| epoch  11 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.98 | loss  6.68 | ppl   798.08\n",
            "| epoch  11 |  1600/ 2928 batches | lr 0.01 | ms/batch 15.95 | loss  6.69 | ppl   801.18\n",
            "| epoch  11 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.74 | loss  6.67 | ppl   787.20\n",
            "| epoch  11 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.73 | loss  6.69 | ppl   804.84\n",
            "| epoch  11 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.14 | loss  6.61 | ppl   740.51\n",
            "| epoch  11 |  2400/ 2928 batches | lr 0.01 | ms/batch 15.96 | loss  6.63 | ppl   759.95\n",
            "| epoch  11 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.72 | loss  6.64 | ppl   761.78\n",
            "| epoch  11 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.77 | loss  6.63 | ppl   759.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 48.83s | valid loss  6.50 | valid ppl   662.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 2928 batches | lr 0.01 | ms/batch 15.90 | loss  6.67 | ppl   791.09\n",
            "| epoch  12 |   400/ 2928 batches | lr 0.01 | ms/batch 15.78 | loss  6.65 | ppl   776.19\n",
            "| epoch  12 |   600/ 2928 batches | lr 0.01 | ms/batch 16.18 | loss  6.61 | ppl   742.55\n",
            "| epoch  12 |   800/ 2928 batches | lr 0.01 | ms/batch 16.02 | loss  6.65 | ppl   776.33\n",
            "| epoch  12 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.80 | loss  6.63 | ppl   754.55\n",
            "| epoch  12 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.72 | loss  6.67 | ppl   785.09\n",
            "| epoch  12 |  1400/ 2928 batches | lr 0.01 | ms/batch 16.09 | loss  6.66 | ppl   780.01\n",
            "| epoch  12 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.07 | loss  6.67 | ppl   785.04\n",
            "| epoch  12 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  6.65 | ppl   770.26\n",
            "| epoch  12 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.67 | ppl   787.44\n",
            "| epoch  12 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.03 | loss  6.59 | ppl   725.16\n",
            "| epoch  12 |  2400/ 2928 batches | lr 0.01 | ms/batch 15.96 | loss  6.61 | ppl   743.60\n",
            "| epoch  12 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.74 | loss  6.62 | ppl   747.86\n",
            "| epoch  12 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.84 | loss  6.61 | ppl   744.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 48.92s | valid loss  6.48 | valid ppl   649.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 2928 batches | lr 0.01 | ms/batch 15.78 | loss  6.65 | ppl   775.90\n",
            "| epoch  13 |   400/ 2928 batches | lr 0.01 | ms/batch 15.80 | loss  6.63 | ppl   760.99\n",
            "| epoch  13 |   600/ 2928 batches | lr 0.01 | ms/batch 16.78 | loss  6.59 | ppl   729.06\n",
            "| epoch  13 |   800/ 2928 batches | lr 0.01 | ms/batch 16.48 | loss  6.64 | ppl   761.42\n",
            "| epoch  13 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.64 | loss  6.61 | ppl   739.62\n",
            "| epoch  13 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.84 | loss  6.65 | ppl   772.14\n",
            "| epoch  13 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.90 | loss  6.64 | ppl   764.76\n",
            "| epoch  13 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.20 | loss  6.65 | ppl   769.93\n",
            "| epoch  13 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.63 | ppl   755.34\n",
            "| epoch  13 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.82 | loss  6.65 | ppl   772.52\n",
            "| epoch  13 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.10 | loss  6.57 | ppl   712.08\n",
            "| epoch  13 |  2400/ 2928 batches | lr 0.01 | ms/batch 15.99 | loss  6.59 | ppl   729.88\n",
            "| epoch  13 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.87 | loss  6.60 | ppl   734.21\n",
            "| epoch  13 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.76 | loss  6.59 | ppl   731.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 49.26s | valid loss  6.46 | valid ppl   638.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 2928 batches | lr 0.01 | ms/batch 15.90 | loss  6.64 | ppl   762.21\n",
            "| epoch  14 |   400/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.62 | ppl   748.05\n",
            "| epoch  14 |   600/ 2928 batches | lr 0.01 | ms/batch 16.02 | loss  6.57 | ppl   715.34\n",
            "| epoch  14 |   800/ 2928 batches | lr 0.01 | ms/batch 16.08 | loss  6.62 | ppl   750.19\n",
            "| epoch  14 |  1000/ 2928 batches | lr 0.01 | ms/batch 15.71 | loss  6.59 | ppl   727.55\n",
            "| epoch  14 |  1200/ 2928 batches | lr 0.01 | ms/batch 15.80 | loss  6.63 | ppl   759.64\n",
            "| epoch  14 |  1400/ 2928 batches | lr 0.01 | ms/batch 15.96 | loss  6.62 | ppl   752.13\n",
            "| epoch  14 |  1600/ 2928 batches | lr 0.01 | ms/batch 16.18 | loss  6.63 | ppl   757.46\n",
            "| epoch  14 |  1800/ 2928 batches | lr 0.01 | ms/batch 15.75 | loss  6.61 | ppl   742.75\n",
            "| epoch  14 |  2000/ 2928 batches | lr 0.01 | ms/batch 15.76 | loss  6.63 | ppl   759.92\n",
            "| epoch  14 |  2200/ 2928 batches | lr 0.01 | ms/batch 16.17 | loss  6.55 | ppl   701.47\n",
            "| epoch  14 |  2400/ 2928 batches | lr 0.01 | ms/batch 15.94 | loss  6.58 | ppl   719.44\n",
            "| epoch  14 |  2600/ 2928 batches | lr 0.01 | ms/batch 15.85 | loss  6.59 | ppl   724.58\n",
            "| epoch  14 |  2800/ 2928 batches | lr 0.01 | ms/batch 15.79 | loss  6.58 | ppl   719.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 48.98s | valid loss  6.44 | valid ppl   627.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.62 | ppl   751.96\n",
            "| epoch  15 |   400/ 2928 batches | lr 0.00 | ms/batch 15.91 | loss  6.60 | ppl   738.36\n",
            "| epoch  15 |   600/ 2928 batches | lr 0.00 | ms/batch 15.87 | loss  6.56 | ppl   707.56\n",
            "| epoch  15 |   800/ 2928 batches | lr 0.00 | ms/batch 16.10 | loss  6.60 | ppl   738.02\n",
            "| epoch  15 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.57 | ppl   716.21\n",
            "| epoch  15 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.62 | ppl   748.35\n",
            "| epoch  15 |  1400/ 2928 batches | lr 0.00 | ms/batch 15.95 | loss  6.61 | ppl   742.01\n",
            "| epoch  15 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.14 | loss  6.62 | ppl   746.87\n",
            "| epoch  15 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.60 | ppl   732.96\n",
            "| epoch  15 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.62 | ppl   750.07\n",
            "| epoch  15 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.23 | loss  6.54 | ppl   690.28\n",
            "| epoch  15 |  2400/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.56 | ppl   708.36\n",
            "| epoch  15 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.57 | ppl   714.27\n",
            "| epoch  15 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.57 | ppl   710.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 48.98s | valid loss  6.43 | valid ppl   619.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 2928 batches | lr 0.00 | ms/batch 15.87 | loss  6.61 | ppl   740.65\n",
            "| epoch  16 |   400/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.59 | ppl   728.02\n",
            "| epoch  16 |   600/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.55 | ppl   697.19\n",
            "| epoch  16 |   800/ 2928 batches | lr 0.00 | ms/batch 16.09 | loss  6.59 | ppl   729.94\n",
            "| epoch  16 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.56 | ppl   707.49\n",
            "| epoch  16 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.60 | ppl   738.28\n",
            "| epoch  16 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.60 | ppl   731.79\n",
            "| epoch  16 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.01 | loss  6.60 | ppl   737.93\n",
            "| epoch  16 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.58 | ppl   722.94\n",
            "| epoch  16 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.67 | loss  6.61 | ppl   739.91\n",
            "| epoch  16 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.14 | loss  6.53 | ppl   682.07\n",
            "| epoch  16 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.91 | loss  6.55 | ppl   700.38\n",
            "| epoch  16 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.56 | ppl   705.58\n",
            "| epoch  16 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.84 | loss  6.55 | ppl   701.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 48.92s | valid loss  6.42 | valid ppl   611.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.59 | ppl   731.12\n",
            "| epoch  17 |   400/ 2928 batches | lr 0.00 | ms/batch 15.69 | loss  6.58 | ppl   721.46\n",
            "| epoch  17 |   600/ 2928 batches | lr 0.00 | ms/batch 16.04 | loss  6.53 | ppl   687.86\n",
            "| epoch  17 |   800/ 2928 batches | lr 0.00 | ms/batch 16.04 | loss  6.58 | ppl   719.50\n",
            "| epoch  17 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.55 | ppl   697.75\n",
            "| epoch  17 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.59 | ppl   730.33\n",
            "| epoch  17 |  1400/ 2928 batches | lr 0.00 | ms/batch 15.95 | loss  6.58 | ppl   724.14\n",
            "| epoch  17 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.65 | loss  6.59 | ppl   729.10\n",
            "| epoch  17 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.57 | ppl   715.03\n",
            "| epoch  17 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.60 | ppl   732.96\n",
            "| epoch  17 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.18 | loss  6.51 | ppl   672.79\n",
            "| epoch  17 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.98 | loss  6.54 | ppl   692.15\n",
            "| epoch  17 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.55 | ppl   697.22\n",
            "| epoch  17 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.54 | ppl   694.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 49.03s | valid loss  6.40 | valid ppl   604.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 2928 batches | lr 0.00 | ms/batch 15.84 | loss  6.58 | ppl   724.04\n",
            "| epoch  18 |   400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.57 | ppl   713.14\n",
            "| epoch  18 |   600/ 2928 batches | lr 0.00 | ms/batch 16.09 | loss  6.52 | ppl   679.70\n",
            "| epoch  18 |   800/ 2928 batches | lr 0.00 | ms/batch 16.17 | loss  6.57 | ppl   713.08\n",
            "| epoch  18 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.54 | ppl   689.99\n",
            "| epoch  18 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.58 | ppl   721.98\n",
            "| epoch  18 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.18 | loss  6.57 | ppl   716.86\n",
            "| epoch  18 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.58 | ppl   721.33\n",
            "| epoch  18 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.56 | ppl   705.68\n",
            "| epoch  18 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.58 | ppl   724.00\n",
            "| epoch  18 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.20 | loss  6.50 | ppl   667.13\n",
            "| epoch  18 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.53 | ppl   683.74\n",
            "| epoch  18 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.54 | ppl   691.38\n",
            "| epoch  18 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.53 | ppl   686.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 48.99s | valid loss  6.39 | valid ppl   598.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.57 | ppl   716.08\n",
            "| epoch  19 |   400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.56 | ppl   706.29\n",
            "| epoch  19 |   600/ 2928 batches | lr 0.00 | ms/batch 15.89 | loss  6.51 | ppl   672.47\n",
            "| epoch  19 |   800/ 2928 batches | lr 0.00 | ms/batch 16.24 | loss  6.56 | ppl   705.16\n",
            "| epoch  19 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.53 | ppl   682.37\n",
            "| epoch  19 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.57 | ppl   714.83\n",
            "| epoch  19 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.07 | loss  6.57 | ppl   710.23\n",
            "| epoch  19 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.13 | loss  6.57 | ppl   714.75\n",
            "| epoch  19 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.55 | ppl   700.00\n",
            "| epoch  19 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.58 | ppl   717.05\n",
            "| epoch  19 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.20 | loss  6.49 | ppl   660.11\n",
            "| epoch  19 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.52 | ppl   677.77\n",
            "| epoch  19 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.53 | ppl   683.70\n",
            "| epoch  19 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.52 | ppl   679.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 48.89s | valid loss  6.38 | valid ppl   592.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.56 | ppl   709.36\n",
            "| epoch  20 |   400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.55 | ppl   700.27\n",
            "| epoch  20 |   600/ 2928 batches | lr 0.00 | ms/batch 16.01 | loss  6.50 | ppl   665.26\n",
            "| epoch  20 |   800/ 2928 batches | lr 0.00 | ms/batch 16.05 | loss  6.55 | ppl   698.70\n",
            "| epoch  20 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.52 | ppl   676.15\n",
            "| epoch  20 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.70 | loss  6.56 | ppl   709.00\n",
            "| epoch  20 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.04 | loss  6.55 | ppl   702.43\n",
            "| epoch  20 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.89 | loss  6.56 | ppl   707.31\n",
            "| epoch  20 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.66 | loss  6.54 | ppl   694.03\n",
            "| epoch  20 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.70 | loss  6.57 | ppl   711.47\n",
            "| epoch  20 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.13 | loss  6.48 | ppl   653.89\n",
            "| epoch  20 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.95 | loss  6.51 | ppl   672.05\n",
            "| epoch  20 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.52 | ppl   678.02\n",
            "| epoch  20 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.66 | loss  6.52 | ppl   675.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 48.76s | valid loss  6.38 | valid ppl   587.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.56 | ppl   704.09\n",
            "| epoch  21 |   400/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.54 | ppl   693.47\n",
            "| epoch  21 |   600/ 2928 batches | lr 0.00 | ms/batch 15.97 | loss  6.49 | ppl   660.65\n",
            "| epoch  21 |   800/ 2928 batches | lr 0.00 | ms/batch 16.14 | loss  6.54 | ppl   694.23\n",
            "| epoch  21 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.69 | loss  6.51 | ppl   670.77\n",
            "| epoch  21 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.55 | ppl   701.81\n",
            "| epoch  21 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.06 | loss  6.55 | ppl   697.16\n",
            "| epoch  21 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.96 | loss  6.55 | ppl   702.14\n",
            "| epoch  21 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.53 | ppl   688.08\n",
            "| epoch  21 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.68 | loss  6.56 | ppl   705.94\n",
            "| epoch  21 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.21 | loss  6.47 | ppl   648.61\n",
            "| epoch  21 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.84 | loss  6.50 | ppl   667.34\n",
            "| epoch  21 |  2600/ 2928 batches | lr 0.00 | ms/batch 16.11 | loss  6.51 | ppl   672.92\n",
            "| epoch  21 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.92 | loss  6.51 | ppl   669.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 48.95s | valid loss  6.37 | valid ppl   582.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.55 | ppl   696.63\n",
            "| epoch  22 |   400/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.53 | ppl   688.05\n",
            "| epoch  22 |   600/ 2928 batches | lr 0.00 | ms/batch 16.06 | loss  6.49 | ppl   656.96\n",
            "| epoch  22 |   800/ 2928 batches | lr 0.00 | ms/batch 15.97 | loss  6.53 | ppl   687.81\n",
            "| epoch  22 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.87 | loss  6.50 | ppl   665.42\n",
            "| epoch  22 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.55 | ppl   696.13\n",
            "| epoch  22 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.18 | loss  6.54 | ppl   692.43\n",
            "| epoch  22 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.55 | ppl   697.65\n",
            "| epoch  22 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.53 | ppl   683.00\n",
            "| epoch  22 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.55 | ppl   700.91\n",
            "| epoch  22 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.40 | loss  6.47 | ppl   643.80\n",
            "| epoch  22 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.49 | ppl   661.43\n",
            "| epoch  22 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.65 | loss  6.50 | ppl   667.99\n",
            "| epoch  22 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.50 | ppl   663.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 48.81s | valid loss  6.36 | valid ppl   578.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.54 | ppl   693.65\n",
            "| epoch  23 |   400/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.53 | ppl   683.50\n",
            "| epoch  23 |   600/ 2928 batches | lr 0.00 | ms/batch 15.99 | loss  6.48 | ppl   649.63\n",
            "| epoch  23 |   800/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.53 | ppl   684.38\n",
            "| epoch  23 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.49 | ppl   660.12\n",
            "| epoch  23 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.54 | ppl   693.45\n",
            "| epoch  23 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.25 | loss  6.53 | ppl   687.36\n",
            "| epoch  23 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.54 | ppl   691.04\n",
            "| epoch  23 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.52 | ppl   677.05\n",
            "| epoch  23 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.84 | loss  6.55 | ppl   695.94\n",
            "| epoch  23 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.25 | loss  6.46 | ppl   638.13\n",
            "| epoch  23 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.49 | ppl   656.24\n",
            "| epoch  23 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.66 | loss  6.50 | ppl   662.59\n",
            "| epoch  23 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.92 | loss  6.49 | ppl   658.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 48.83s | valid loss  6.35 | valid ppl   574.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 2928 batches | lr 0.00 | ms/batch 15.89 | loss  6.53 | ppl   687.43\n",
            "| epoch  24 |   400/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.52 | ppl   680.54\n",
            "| epoch  24 |   600/ 2928 batches | lr 0.00 | ms/batch 16.05 | loss  6.47 | ppl   645.80\n",
            "| epoch  24 |   800/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.52 | ppl   679.21\n",
            "| epoch  24 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.49 | ppl   655.46\n",
            "| epoch  24 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.53 | ppl   687.08\n",
            "| epoch  24 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.16 | loss  6.53 | ppl   683.41\n",
            "| epoch  24 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.91 | loss  6.53 | ppl   687.04\n",
            "| epoch  24 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.51 | ppl   673.27\n",
            "| epoch  24 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.54 | ppl   690.83\n",
            "| epoch  24 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.36 | loss  6.45 | ppl   635.23\n",
            "| epoch  24 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.48 | ppl   653.36\n",
            "| epoch  24 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.49 | ppl   659.01\n",
            "| epoch  24 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.48 | ppl   654.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 48.93s | valid loss  6.35 | valid ppl   570.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 2928 batches | lr 0.00 | ms/batch 15.96 | loss  6.53 | ppl   682.25\n",
            "| epoch  25 |   400/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.51 | ppl   674.88\n",
            "| epoch  25 |   600/ 2928 batches | lr 0.00 | ms/batch 15.98 | loss  6.46 | ppl   642.05\n",
            "| epoch  25 |   800/ 2928 batches | lr 0.00 | ms/batch 16.09 | loss  6.51 | ppl   674.43\n",
            "| epoch  25 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.48 | ppl   649.96\n",
            "| epoch  25 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.53 | ppl   683.80\n",
            "| epoch  25 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.22 | loss  6.52 | ppl   679.32\n",
            "| epoch  25 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.93 | loss  6.53 | ppl   683.36\n",
            "| epoch  25 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.69 | loss  6.51 | ppl   669.84\n",
            "| epoch  25 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.54 | ppl   689.03\n",
            "| epoch  25 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.35 | loss  6.45 | ppl   630.35\n",
            "| epoch  25 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.48 | ppl   649.36\n",
            "| epoch  25 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.65 | loss  6.48 | ppl   654.54\n",
            "| epoch  25 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.48 | ppl   650.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 48.92s | valid loss  6.34 | valid ppl   567.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.52 | ppl   679.44\n",
            "| epoch  26 |   400/ 2928 batches | lr 0.00 | ms/batch 16.16 | loss  6.51 | ppl   671.42\n",
            "| epoch  26 |   600/ 2928 batches | lr 0.00 | ms/batch 16.39 | loss  6.46 | ppl   637.40\n",
            "| epoch  26 |   800/ 2928 batches | lr 0.00 | ms/batch 15.97 | loss  6.51 | ppl   670.16\n",
            "| epoch  26 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.47 | ppl   647.69\n",
            "| epoch  26 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.85 | loss  6.52 | ppl   679.72\n",
            "| epoch  26 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.32 | loss  6.52 | ppl   675.47\n",
            "| epoch  26 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.52 | ppl   679.51\n",
            "| epoch  26 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.50 | ppl   667.11\n",
            "| epoch  26 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.53 | ppl   683.42\n",
            "| epoch  26 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.34 | loss  6.44 | ppl   626.62\n",
            "| epoch  26 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.91 | loss  6.47 | ppl   645.20\n",
            "| epoch  26 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.48 | ppl   650.83\n",
            "| epoch  26 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.47 | ppl   647.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 49.12s | valid loss  6.34 | valid ppl   564.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 2928 batches | lr 0.00 | ms/batch 15.90 | loss  6.51 | ppl   675.07\n",
            "| epoch  27 |   400/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.50 | ppl   667.56\n",
            "| epoch  27 |   600/ 2928 batches | lr 0.00 | ms/batch 16.06 | loss  6.45 | ppl   634.22\n",
            "| epoch  27 |   800/ 2928 batches | lr 0.00 | ms/batch 16.05 | loss  6.50 | ppl   667.30\n",
            "| epoch  27 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.88 | loss  6.47 | ppl   644.69\n",
            "| epoch  27 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.88 | loss  6.52 | ppl   675.33\n",
            "| epoch  27 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.15 | loss  6.51 | ppl   671.43\n",
            "| epoch  27 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.97 | loss  6.52 | ppl   676.57\n",
            "| epoch  27 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.50 | ppl   662.47\n",
            "| epoch  27 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.90 | loss  6.52 | ppl   680.64\n",
            "| epoch  27 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.40 | loss  6.44 | ppl   623.54\n",
            "| epoch  27 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.46 | ppl   641.81\n",
            "| epoch  27 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.48 | ppl   650.00\n",
            "| epoch  27 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.88 | loss  6.47 | ppl   643.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 49.06s | valid loss  6.33 | valid ppl   561.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 2928 batches | lr 0.00 | ms/batch 15.96 | loss  6.51 | ppl   671.90\n",
            "| epoch  28 |   400/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.50 | ppl   663.70\n",
            "| epoch  28 |   600/ 2928 batches | lr 0.00 | ms/batch 16.16 | loss  6.45 | ppl   632.55\n",
            "| epoch  28 |   800/ 2928 batches | lr 0.00 | ms/batch 16.21 | loss  6.50 | ppl   664.92\n",
            "| epoch  28 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.46 | ppl   640.80\n",
            "| epoch  28 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.92 | loss  6.51 | ppl   672.10\n",
            "| epoch  28 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.43 | loss  6.50 | ppl   668.02\n",
            "| epoch  28 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.88 | loss  6.51 | ppl   673.13\n",
            "| epoch  28 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.70 | loss  6.49 | ppl   659.33\n",
            "| epoch  28 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.85 | loss  6.52 | ppl   675.85\n",
            "| epoch  28 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.54 | loss  6.43 | ppl   621.23\n",
            "| epoch  28 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.46 | ppl   638.26\n",
            "| epoch  28 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.47 | ppl   643.71\n",
            "| epoch  28 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.46 | ppl   641.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 49.05s | valid loss  6.33 | valid ppl   558.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.51 | ppl   669.51\n",
            "| epoch  29 |   400/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.49 | ppl   661.68\n",
            "| epoch  29 |   600/ 2928 batches | lr 0.00 | ms/batch 16.12 | loss  6.44 | ppl   628.65\n",
            "| epoch  29 |   800/ 2928 batches | lr 0.00 | ms/batch 16.20 | loss  6.49 | ppl   660.96\n",
            "| epoch  29 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.46 | ppl   636.43\n",
            "| epoch  29 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.88 | loss  6.51 | ppl   670.17\n",
            "| epoch  29 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.30 | loss  6.50 | ppl   666.03\n",
            "| epoch  29 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.51 | ppl   671.11\n",
            "| epoch  29 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.49 | ppl   656.87\n",
            "| epoch  29 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.51 | ppl   673.89\n",
            "| epoch  29 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.29 | loss  6.43 | ppl   618.02\n",
            "| epoch  29 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.46 | ppl   636.96\n",
            "| epoch  29 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.47 | ppl   642.46\n",
            "| epoch  29 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.46 | ppl   637.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 49.01s | valid loss  6.32 | valid ppl   556.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.50 | ppl   665.59\n",
            "| epoch  30 |   400/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.49 | ppl   659.34\n",
            "| epoch  30 |   600/ 2928 batches | lr 0.00 | ms/batch 16.23 | loss  6.44 | ppl   625.42\n",
            "| epoch  30 |   800/ 2928 batches | lr 0.00 | ms/batch 16.00 | loss  6.49 | ppl   658.89\n",
            "| epoch  30 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.70 | loss  6.45 | ppl   634.89\n",
            "| epoch  30 |  1200/ 2928 batches | lr 0.00 | ms/batch 16.32 | loss  6.50 | ppl   665.95\n",
            "| epoch  30 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.34 | loss  6.50 | ppl   663.11\n",
            "| epoch  30 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.50 | ppl   667.37\n",
            "| epoch  30 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.70 | loss  6.48 | ppl   653.31\n",
            "| epoch  30 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.51 | ppl   670.00\n",
            "| epoch  30 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.47 | loss  6.42 | ppl   615.75\n",
            "| epoch  30 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.45 | ppl   633.00\n",
            "| epoch  30 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.46 | ppl   640.24\n",
            "| epoch  30 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.46 | ppl   635.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 48.94s | valid loss  6.32 | valid ppl   553.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 2928 batches | lr 0.00 | ms/batch 15.84 | loss  6.49 | ppl   661.66\n",
            "| epoch  31 |   400/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.49 | ppl   655.32\n",
            "| epoch  31 |   600/ 2928 batches | lr 0.00 | ms/batch 16.16 | loss  6.43 | ppl   621.40\n",
            "| epoch  31 |   800/ 2928 batches | lr 0.00 | ms/batch 15.94 | loss  6.49 | ppl   655.56\n",
            "| epoch  31 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.45 | ppl   632.24\n",
            "| epoch  31 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.50 | ppl   664.76\n",
            "| epoch  31 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.30 | loss  6.49 | ppl   660.91\n",
            "| epoch  31 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.50 | ppl   664.98\n",
            "| epoch  31 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.48 | ppl   651.26\n",
            "| epoch  31 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.51 | ppl   668.53\n",
            "| epoch  31 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.21 | loss  6.42 | ppl   612.13\n",
            "| epoch  31 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.45 | ppl   631.61\n",
            "| epoch  31 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.46 | ppl   638.06\n",
            "| epoch  31 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.45 | ppl   632.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 48.79s | valid loss  6.31 | valid ppl   551.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.49 | ppl   661.36\n",
            "| epoch  32 |   400/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.48 | ppl   653.87\n",
            "| epoch  32 |   600/ 2928 batches | lr 0.00 | ms/batch 15.95 | loss  6.43 | ppl   620.66\n",
            "| epoch  32 |   800/ 2928 batches | lr 0.00 | ms/batch 16.18 | loss  6.48 | ppl   652.67\n",
            "| epoch  32 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.44 | ppl   628.77\n",
            "| epoch  32 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.49 | ppl   661.58\n",
            "| epoch  32 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.21 | loss  6.49 | ppl   657.33\n",
            "| epoch  32 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.96 | loss  6.49 | ppl   661.65\n",
            "| epoch  32 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.64 | loss  6.47 | ppl   648.49\n",
            "| epoch  32 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.50 | ppl   665.49\n",
            "| epoch  32 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.32 | loss  6.41 | ppl   610.70\n",
            "| epoch  32 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.44 | ppl   628.87\n",
            "| epoch  32 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.45 | ppl   634.28\n",
            "| epoch  32 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.44 | ppl   629.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 48.80s | valid loss  6.31 | valid ppl   549.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 2928 batches | lr 0.00 | ms/batch 15.85 | loss  6.49 | ppl   656.89\n",
            "| epoch  33 |   400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.48 | ppl   651.48\n",
            "| epoch  33 |   600/ 2928 batches | lr 0.00 | ms/batch 15.97 | loss  6.42 | ppl   617.00\n",
            "| epoch  33 |   800/ 2928 batches | lr 0.00 | ms/batch 16.19 | loss  6.48 | ppl   650.39\n",
            "| epoch  33 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.44 | ppl   626.92\n",
            "| epoch  33 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.49 | ppl   659.68\n",
            "| epoch  33 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.17 | loss  6.48 | ppl   654.64\n",
            "| epoch  33 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.01 | loss  6.49 | ppl   660.49\n",
            "| epoch  33 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.47 | ppl   646.32\n",
            "| epoch  33 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.50 | ppl   662.38\n",
            "| epoch  33 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.25 | loss  6.41 | ppl   607.14\n",
            "| epoch  33 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.44 | ppl   626.04\n",
            "| epoch  33 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.45 | ppl   631.78\n",
            "| epoch  33 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.44 | ppl   627.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 48.89s | valid loss  6.31 | valid ppl   547.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.49 | ppl   655.81\n",
            "| epoch  34 |   400/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.48 | ppl   649.93\n",
            "| epoch  34 |   600/ 2928 batches | lr 0.00 | ms/batch 16.04 | loss  6.42 | ppl   615.42\n",
            "| epoch  34 |   800/ 2928 batches | lr 0.00 | ms/batch 16.16 | loss  6.47 | ppl   648.67\n",
            "| epoch  34 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.44 | ppl   623.87\n",
            "| epoch  34 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.49 | ppl   656.31\n",
            "| epoch  34 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.17 | loss  6.48 | ppl   652.65\n",
            "| epoch  34 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.06 | loss  6.49 | ppl   657.27\n",
            "| epoch  34 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.47 | ppl   643.79\n",
            "| epoch  34 |  2000/ 2928 batches | lr 0.00 | ms/batch 16.13 | loss  6.49 | ppl   659.91\n",
            "| epoch  34 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.59 | loss  6.41 | ppl   605.33\n",
            "| epoch  34 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.44 | ppl   623.53\n",
            "| epoch  34 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.45 | ppl   630.72\n",
            "| epoch  34 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.44 | ppl   625.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 49.04s | valid loss  6.30 | valid ppl   545.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 2928 batches | lr 0.00 | ms/batch 15.96 | loss  6.48 | ppl   653.81\n",
            "| epoch  35 |   400/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.47 | ppl   646.26\n",
            "| epoch  35 |   600/ 2928 batches | lr 0.00 | ms/batch 16.04 | loss  6.42 | ppl   612.66\n",
            "| epoch  35 |   800/ 2928 batches | lr 0.00 | ms/batch 16.13 | loss  6.47 | ppl   647.65\n",
            "| epoch  35 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.43 | ppl   622.22\n",
            "| epoch  35 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.84 | loss  6.48 | ppl   654.12\n",
            "| epoch  35 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.30 | loss  6.48 | ppl   651.99\n",
            "| epoch  35 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.93 | loss  6.48 | ppl   655.08\n",
            "| epoch  35 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.46 | ppl   641.96\n",
            "| epoch  35 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.49 | ppl   658.36\n",
            "| epoch  35 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.32 | loss  6.40 | ppl   603.35\n",
            "| epoch  35 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.43 | ppl   623.12\n",
            "| epoch  35 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.44 | ppl   629.40\n",
            "| epoch  35 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.43 | ppl   623.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 48.97s | valid loss  6.30 | valid ppl   543.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.48 | ppl   650.47\n",
            "| epoch  36 |   400/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.47 | ppl   645.15\n",
            "| epoch  36 |   600/ 2928 batches | lr 0.00 | ms/batch 16.13 | loss  6.42 | ppl   611.51\n",
            "| epoch  36 |   800/ 2928 batches | lr 0.00 | ms/batch 16.05 | loss  6.47 | ppl   644.12\n",
            "| epoch  36 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.69 | loss  6.43 | ppl   619.63\n",
            "| epoch  36 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.69 | loss  6.48 | ppl   652.28\n",
            "| epoch  36 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.15 | loss  6.48 | ppl   649.10\n",
            "| epoch  36 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.48 | ppl   651.43\n",
            "| epoch  36 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.46 | ppl   639.86\n",
            "| epoch  36 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.49 | ppl   657.33\n",
            "| epoch  36 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.33 | loss  6.40 | ppl   601.86\n",
            "| epoch  36 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.43 | ppl   619.55\n",
            "| epoch  36 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.44 | ppl   626.24\n",
            "| epoch  36 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.43 | ppl   621.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 48.94s | valid loss  6.30 | valid ppl   542.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.47 | ppl   648.05\n",
            "| epoch  37 |   400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.46 | ppl   642.17\n",
            "| epoch  37 |   600/ 2928 batches | lr 0.00 | ms/batch 15.98 | loss  6.41 | ppl   609.85\n",
            "| epoch  37 |   800/ 2928 batches | lr 0.00 | ms/batch 16.05 | loss  6.47 | ppl   643.63\n",
            "| epoch  37 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.43 | ppl   617.78\n",
            "| epoch  37 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.48 | ppl   651.85\n",
            "| epoch  37 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.12 | loss  6.47 | ppl   646.40\n",
            "| epoch  37 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.87 | loss  6.48 | ppl   651.73\n",
            "| epoch  37 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.46 | ppl   639.25\n",
            "| epoch  37 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.49 | ppl   655.34\n",
            "| epoch  37 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.23 | loss  6.40 | ppl   599.73\n",
            "| epoch  37 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.43 | ppl   620.09\n",
            "| epoch  37 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.67 | loss  6.44 | ppl   625.52\n",
            "| epoch  37 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.43 | ppl   619.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 48.89s | valid loss  6.29 | valid ppl   540.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.47 | ppl   646.33\n",
            "| epoch  38 |   400/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.46 | ppl   641.33\n",
            "| epoch  38 |   600/ 2928 batches | lr 0.00 | ms/batch 15.92 | loss  6.41 | ppl   608.14\n",
            "| epoch  38 |   800/ 2928 batches | lr 0.00 | ms/batch 16.03 | loss  6.46 | ppl   640.88\n",
            "| epoch  38 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.88 | loss  6.42 | ppl   616.93\n",
            "| epoch  38 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.48 | ppl   649.72\n",
            "| epoch  38 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.01 | loss  6.47 | ppl   645.34\n",
            "| epoch  38 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.05 | loss  6.48 | ppl   649.56\n",
            "| epoch  38 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.46 | ppl   636.84\n",
            "| epoch  38 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.90 | loss  6.48 | ppl   652.99\n",
            "| epoch  38 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.17 | loss  6.39 | ppl   598.45\n",
            "| epoch  38 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.43 | ppl   617.44\n",
            "| epoch  38 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.44 | ppl   624.45\n",
            "| epoch  38 |  2800/ 2928 batches | lr 0.00 | ms/batch 16.33 | loss  6.43 | ppl   618.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 49.01s | valid loss  6.29 | valid ppl   539.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.47 | ppl   644.66\n",
            "| epoch  39 |   400/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.46 | ppl   639.25\n",
            "| epoch  39 |   600/ 2928 batches | lr 0.00 | ms/batch 16.10 | loss  6.41 | ppl   605.77\n",
            "| epoch  39 |   800/ 2928 batches | lr 0.00 | ms/batch 16.15 | loss  6.46 | ppl   638.83\n",
            "| epoch  39 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.42 | ppl   616.13\n",
            "| epoch  39 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.47 | ppl   646.54\n",
            "| epoch  39 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.08 | loss  6.47 | ppl   645.11\n",
            "| epoch  39 |  1600/ 2928 batches | lr 0.00 | ms/batch 16.03 | loss  6.48 | ppl   648.90\n",
            "| epoch  39 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.82 | loss  6.45 | ppl   634.88\n",
            "| epoch  39 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.88 | loss  6.48 | ppl   651.61\n",
            "| epoch  39 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.26 | loss  6.39 | ppl   597.81\n",
            "| epoch  39 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.42 | ppl   615.64\n",
            "| epoch  39 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.43 | ppl   622.77\n",
            "| epoch  39 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.43 | ppl   617.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 49.02s | valid loss  6.29 | valid ppl   537.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 2928 batches | lr 0.00 | ms/batch 15.89 | loss  6.47 | ppl   644.39\n",
            "| epoch  40 |   400/ 2928 batches | lr 0.00 | ms/batch 15.70 | loss  6.46 | ppl   637.36\n",
            "| epoch  40 |   600/ 2928 batches | lr 0.00 | ms/batch 16.11 | loss  6.41 | ppl   605.12\n",
            "| epoch  40 |   800/ 2928 batches | lr 0.00 | ms/batch 16.18 | loss  6.46 | ppl   637.55\n",
            "| epoch  40 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.42 | ppl   614.09\n",
            "| epoch  40 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.47 | ppl   645.86\n",
            "| epoch  40 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.14 | loss  6.46 | ppl   641.37\n",
            "| epoch  40 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.95 | loss  6.47 | ppl   648.18\n",
            "| epoch  40 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.45 | ppl   632.19\n",
            "| epoch  40 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.90 | loss  6.48 | ppl   650.97\n",
            "| epoch  40 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.32 | loss  6.39 | ppl   596.04\n",
            "| epoch  40 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.42 | ppl   614.82\n",
            "| epoch  40 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.43 | ppl   619.23\n",
            "| epoch  40 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.90 | loss  6.42 | ppl   615.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 49.03s | valid loss  6.29 | valid ppl   536.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/ 2928 batches | lr 0.00 | ms/batch 16.02 | loss  6.47 | ppl   642.38\n",
            "| epoch  41 |   400/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.46 | ppl   637.10\n",
            "| epoch  41 |   600/ 2928 batches | lr 0.00 | ms/batch 16.10 | loss  6.40 | ppl   602.99\n",
            "| epoch  41 |   800/ 2928 batches | lr 0.00 | ms/batch 16.27 | loss  6.46 | ppl   637.18\n",
            "| epoch  41 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.42 | ppl   612.03\n",
            "| epoch  41 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.47 | ppl   645.06\n",
            "| epoch  41 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.15 | loss  6.46 | ppl   641.02\n",
            "| epoch  41 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.91 | loss  6.47 | ppl   645.32\n",
            "| epoch  41 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.45 | ppl   631.71\n",
            "| epoch  41 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.47 | ppl   648.38\n",
            "| epoch  41 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.29 | loss  6.39 | ppl   593.25\n",
            "| epoch  41 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.83 | loss  6.42 | ppl   612.52\n",
            "| epoch  41 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.43 | ppl   619.64\n",
            "| epoch  41 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.80 | loss  6.42 | ppl   614.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 49.05s | valid loss  6.28 | valid ppl   535.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/ 2928 batches | lr 0.00 | ms/batch 15.84 | loss  6.46 | ppl   641.41\n",
            "| epoch  42 |   400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.45 | ppl   634.39\n",
            "| epoch  42 |   600/ 2928 batches | lr 0.00 | ms/batch 15.99 | loss  6.40 | ppl   602.41\n",
            "| epoch  42 |   800/ 2928 batches | lr 0.00 | ms/batch 16.18 | loss  6.45 | ppl   633.99\n",
            "| epoch  42 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.42 | ppl   611.19\n",
            "| epoch  42 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.79 | loss  6.47 | ppl   642.97\n",
            "| epoch  42 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.10 | loss  6.46 | ppl   639.67\n",
            "| epoch  42 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.99 | loss  6.47 | ppl   643.93\n",
            "| epoch  42 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.89 | loss  6.45 | ppl   632.03\n",
            "| epoch  42 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.47 | ppl   647.49\n",
            "| epoch  42 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.31 | loss  6.38 | ppl   592.18\n",
            "| epoch  42 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.73 | loss  6.41 | ppl   610.94\n",
            "| epoch  42 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.43 | ppl   618.00\n",
            "| epoch  42 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.42 | ppl   612.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 48.99s | valid loss  6.28 | valid ppl   534.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.46 | ppl   640.49\n",
            "| epoch  43 |   400/ 2928 batches | lr 0.00 | ms/batch 16.00 | loss  6.45 | ppl   634.99\n",
            "| epoch  43 |   600/ 2928 batches | lr 0.00 | ms/batch 16.40 | loss  6.40 | ppl   600.46\n",
            "| epoch  43 |   800/ 2928 batches | lr 0.00 | ms/batch 16.10 | loss  6.45 | ppl   634.16\n",
            "| epoch  43 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.41 | ppl   608.87\n",
            "| epoch  43 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.46 | ppl   641.06\n",
            "| epoch  43 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.19 | loss  6.46 | ppl   639.09\n",
            "| epoch  43 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.98 | loss  6.47 | ppl   643.56\n",
            "| epoch  43 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.77 | loss  6.45 | ppl   629.64\n",
            "| epoch  43 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.66 | loss  6.47 | ppl   645.93\n",
            "| epoch  43 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.34 | loss  6.38 | ppl   591.31\n",
            "| epoch  43 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.42 | ppl   611.20\n",
            "| epoch  43 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.69 | loss  6.42 | ppl   616.36\n",
            "| epoch  43 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.72 | loss  6.42 | ppl   611.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 49.01s | valid loss  6.28 | valid ppl   532.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.46 | ppl   637.93\n",
            "| epoch  44 |   400/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.45 | ppl   632.48\n",
            "| epoch  44 |   600/ 2928 batches | lr 0.00 | ms/batch 16.11 | loss  6.40 | ppl   599.98\n",
            "| epoch  44 |   800/ 2928 batches | lr 0.00 | ms/batch 16.04 | loss  6.45 | ppl   633.24\n",
            "| epoch  44 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.41 | ppl   608.74\n",
            "| epoch  44 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.86 | loss  6.46 | ppl   640.21\n",
            "| epoch  44 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.17 | loss  6.46 | ppl   637.09\n",
            "| epoch  44 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.78 | loss  6.47 | ppl   642.38\n",
            "| epoch  44 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.81 | loss  6.44 | ppl   628.02\n",
            "| epoch  44 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.47 | ppl   644.41\n",
            "| epoch  44 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.42 | loss  6.38 | ppl   591.72\n",
            "| epoch  44 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.67 | loss  6.41 | ppl   610.31\n",
            "| epoch  44 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.42 | ppl   616.07\n",
            "| epoch  44 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.71 | loss  6.41 | ppl   610.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 48.91s | valid loss  6.28 | valid ppl   532.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.45 | ppl   635.74\n",
            "| epoch  45 |   400/ 2928 batches | lr 0.00 | ms/batch 15.64 | loss  6.45 | ppl   631.84\n",
            "| epoch  45 |   600/ 2928 batches | lr 0.00 | ms/batch 16.10 | loss  6.39 | ppl   598.62\n",
            "| epoch  45 |   800/ 2928 batches | lr 0.00 | ms/batch 16.04 | loss  6.45 | ppl   632.15\n",
            "| epoch  45 |  1000/ 2928 batches | lr 0.00 | ms/batch 15.74 | loss  6.41 | ppl   607.58\n",
            "| epoch  45 |  1200/ 2928 batches | lr 0.00 | ms/batch 15.68 | loss  6.46 | ppl   637.74\n",
            "| epoch  45 |  1400/ 2928 batches | lr 0.00 | ms/batch 16.01 | loss  6.46 | ppl   636.03\n",
            "| epoch  45 |  1600/ 2928 batches | lr 0.00 | ms/batch 15.84 | loss  6.46 | ppl   640.20\n",
            "| epoch  45 |  1800/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.44 | ppl   628.03\n",
            "| epoch  45 |  2000/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.47 | ppl   643.84\n",
            "| epoch  45 |  2200/ 2928 batches | lr 0.00 | ms/batch 16.42 | loss  6.38 | ppl   589.14\n",
            "| epoch  45 |  2400/ 2928 batches | lr 0.00 | ms/batch 15.75 | loss  6.41 | ppl   607.22\n",
            "| epoch  45 |  2600/ 2928 batches | lr 0.00 | ms/batch 15.63 | loss  6.42 | ppl   614.13\n",
            "| epoch  45 |  2800/ 2928 batches | lr 0.00 | ms/batch 15.76 | loss  6.41 | ppl   608.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 48.84s | valid loss  6.27 | valid ppl   531.12\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "Your Code Here\n",
        "\"\"\"\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 45\n",
        "best_model = None\n",
        "bptt = 35\n",
        "train_loss_per_epoch = []\n",
        "val_loss_per_epoch = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_loss = train(model, bptt=35)\n",
        "\n",
        "    val_loss = evaluate(model, val_data_b)\n",
        "    train_loss_per_epoch.append(epoch_loss)\n",
        "    val_loss_per_epoch.append(val_loss)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()\n",
        "x_axis = list(range(1, epochs + 1))\n",
        "plt.figure()\n",
        "plt.plot(x_axis, train_loss_per_epoch, label='Train Loss')\n",
        "plt.plot(x_axis, val_loss_per_epoch, label='Val Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel(\"#No of Epochs\")\n",
        "plt.title('Training Loss and Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(x_axis, np.exp(train_loss_per_epoch), label='Train PPL')\n",
        "plt.plot(x_axis, np.exp(val_loss_per_epoch), label='Val PPL')\n",
        "plt.ylabel('PPL')\n",
        "plt.xlabel(\"#No of Epochs\")\n",
        "plt.title('Training PPL and Validation PPL')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shtb9DJ0OrM9"
      },
      "source": [
        "### <img src=\"https://img.icons8.com/color/48/000000/code.png\" style=\"height:50px;display:inline\"> Task 5  - Generate Sentences\n",
        "---\n",
        "Use the following function to generate 3 sentences of length 20, and print them. Do they make sense? (you can compare generated sentences over epochs, to see if some logic is gained during training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w49PKvekOrM9"
      },
      "outputs": [],
      "source": [
        "def generate(model, vocab, nwords=100, temp=1.0):\n",
        "    model.eval()\n",
        "    ntokens = len(vocab)\n",
        "    itos = vocab.vocab.get_itos()\n",
        "    model_input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "    words = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(nwords):\n",
        "            output = model(model_input, None)\n",
        "            word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "            model_input = torch.cat([model_input, word_tensor], 0)\n",
        "            word = itos[word_idx]\n",
        "            words.append(word)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEg9d2p3OrM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778fe8cd-3f16-47dd-9b95-f4b47c928ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ". the allah included is black topical zealand howson of the lake has been moving to the samantha is back\n",
            ", the ciara so it was as not prussia by the due to proposed of the original san day .\n",
            ". the missouri combat removed ' s world expressed of a endgames speculations part . . the mm surrogate by\n"
          ]
        }
      ],
      "source": [
        "num_sentences = 3\n",
        "for i in range(num_sentences):\n",
        "    sentence = generate(model, vocab, nwords=20)\n",
        "    sentence_str = ' '\n",
        "    print(sentence_str.join(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ouput has some structural logic and attention between adjacent words but fails to generate real human-like sentences. After training the model for some epochs we saw the output is making more sense but reaches it's limit very quickly."
      ],
      "metadata": {
        "id": "iwER4_qZ7Jh6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEZB-ZJTOrM9"
      },
      "source": [
        "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
        "---\n",
        "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
        "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
        "* Datasets from <a href=\"https://www.kaggle.com/\">Kaggle</a> - https://www.kaggle.com/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8722dfc8"
      },
      "source": [
        "# Trying to find a compatible torchtext version for torch 2.6.0\n",
        "# Based on torchtext documentation and release notes, torchtext 0.18.0 should be compatible with torch 2.3.0 and above.\n",
        "# However, the error suggests a mismatch. Let's try downgrading torchtext to a slightly older version that might work with torch 2.6.0.\n",
        "# torchtext 0.17.0 was released around the same time as torch 2.2.0, which might be closer to 2.6.0 in terms of JIT symbols.\n",
        "# Let's try installing torchtext 0.17.0 without dependencies to avoid upgrading torch.\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}